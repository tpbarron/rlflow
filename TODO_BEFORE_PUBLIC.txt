To Fix:
* Unify policy iter / value iter with optimize()/ train() organization
* Update TDLambda to have arbitrary policy?
* broken bandit env

Misc:
* logging functionality for tensorboard
* parallel env rollouts for batch policies
* need to be able to define an input / ouput preprocessor
    so that one can easily define how the env observation get's handled over time

Add linear methods:
* RBF

Add gradient methods
* Cross entropy
* Natural policy gradient
* Deterministic policy gradient
* A3C
* Policy Grad Q learning

Well written readme and docs

General Project Outline:
* Algorithms:
  * grad
    * finite difference
    * vanilla pg
    * natural pg
    * deterministic pg
    * cross entropy method
  * dp
    * value iteration - http://www.cs.ubc.ca/~poole/demos/mdp/vi.html
    * policy iteration
  * TD
    * TD(lambda)
    * Q-learning
    * SARSA


Examples
 * DQN breakout
 * PolicyGradient pong
 * CEM

Standardize rewards at unit normal

Notes:
* Any algorithm should run with any policy, and any environment
* If using function approximator, then needs to return gradient
* If using tabular representation, needs then do gradient update on the table?
