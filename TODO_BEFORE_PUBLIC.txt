To Fix:
* Unify policy iter / value iter with optimize()/ train() organization
* Update TDLambda to have arbitrary policy?
* broken bandit env

Misc:
* tensorflow cleanup
* logging functionality for tensorboard
* parallel env rollouts for batch policies

Add function approximators
* Arbitrary neural network

Add linear methods:
* RBF

Add gradient methods
* Cross entropy
* Reinforce policy gradient
* Natural policy gradient
* Deterministic policy gradient
* A3C
* Policy Grad Q learning

Well written readme and docs

General Project Outline:
* Needs optimized tensorflow code so can run on GPU in parallel
* Algorithms:
  * grad
    * finite difference
    * vanilla pg
    * natural pg
    * deterministic pg
    * cross entropy method
  * dp
    * value iteration - http://www.cs.ubc.ca/~poole/demos/mdp/vi.html
    * policy iteration
  * TD
    * TD(lambda)
    * Q-learning
    * SARSA
* Policies
  * Approximators
    * neural network
    * linear
  * tabular


Examples
 * DQN breakout
 * PolicyGradient pong
 * CEM 

Standardize rewards at unit normal

Notes:
* Any algorithm should run with any policy, and any environment
* If using function approximator, then needs to return gradient
* If using tabular representation, needs then do gradient update on the table?
